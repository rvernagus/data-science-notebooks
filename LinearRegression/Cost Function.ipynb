{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.datasets import load_boston\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def standardize(x):\n",
    "    return (x - np.mean(x)) / np.std(x)\n",
    "\n",
    "\n",
    "boston = load_boston()\n",
    "dataset = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "dataset['target'] = boston.target\n",
    "x_range = [dataset['RM'].min(), dataset['RM'].max()]\n",
    "y_range = [dataset['target'].min(), dataset['target'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing the Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([9.5, 8.5, 8.0, 7.0, 6.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_cost(v, e):\n",
    "    return np.sum((v - e)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 7.300000\n",
      "         Iterations: 44\n",
      "         Function evaluations: 88\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin\n",
    "\n",
    "xopt = fmin(squared_cost, x0=0, xtol=1e-8, args=(x,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of optimization is 7.80000000000001\n",
      "The mean is 7.8\n"
     ]
    }
   ],
   "source": [
    "print('The result of optimization is {0}'.format(xopt[0]))\n",
    "print('The mean is {0}'.format(np.mean(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 5.000000\n",
      "         Iterations: 44\n",
      "         Function evaluations: 88\n"
     ]
    }
   ],
   "source": [
    "def absolute_cost(v, e):\n",
    "    return np.sum(np.abs(v - e))\n",
    "\n",
    "xopt = fmin(absolute_cost, x0=0, xtol=1e-8, args=(x,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of optimization is 8.000000000000009\n",
      "The median is 8.0\n"
     ]
    }
   ],
   "source": [
    "print('The result of optimization is {0}'.format(xopt[0]))\n",
    "print('The median is {0}'.format(np.median(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudoinverse and Other Optimization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.10210898 -34.67062078]\n",
      "[  9.10210898 -34.67062078]\n",
      "[  9.10210898 -34.67062078]\n"
     ]
    }
   ],
   "source": [
    "observations = len(dataset)\n",
    "X = dataset['RM'].values.reshape((observations, 1)) # X should always be a matrix, never a vector\n",
    "Xb = np.column_stack((X, np.ones(observations))) # We add the bias\n",
    "y = dataset['target'].values # y can be a vector\n",
    "\n",
    "def matrix_inverse(X, y, pseudo=False):\n",
    "    if pseudo:\n",
    "        return np.dot(np.linalg.pinv(np.dot(X.T, X)), np.dot(X.T, y))\n",
    "    else:\n",
    "        return np.dot(np.linalg.inv(np.dot(X.T, X)), np.dot(X.T, y))\n",
    "\n",
    "def normal_equations(X, y):\n",
    "    return np.linalg.solve(np.dot(X.T, X), np.dot(X.T, y))\n",
    "\n",
    "print(matrix_inverse(Xb, y))\n",
    "print(matrix_inverse(Xb, y, pseudo=True))\n",
    "print(normal_equations(Xb, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize vector w with random numbers taken from a standardized normal curve (0 mean and 0 variance).\n",
    "\n",
    "Update the values of w until the cost, J(w), is small enough to let us know we have reached the optimum minimum.\n",
    "\n",
    "Update the coefficients one-by-one by subtracting each from a portion alpha (learning rate) of the partial derivative of the cost function:\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Implementation of Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = len(dataset)\n",
    "X = dataset['RM'].values.reshape((observations, 1)) # X should always be a matrix\n",
    "X = np.column_stack((X, np.ones(observations))) # Add the bias\n",
    "y = dataset['target'].values # y is a vector\n",
    "\n",
    "import random\n",
    "\n",
    "def random_w(p):\n",
    "    return np.array([np.random.normal() for j in range(p)])\n",
    "\n",
    "def hypothesis(X, w):\n",
    "    return np.dot(X, w)\n",
    "\n",
    "def loss(X, w, y):\n",
    "    return hypothesis(X, w) - y\n",
    "\n",
    "def squared_loss(X, w, y):\n",
    "    return loss(X, w, y)**2\n",
    "\n",
    "def gradient(X, w, y):\n",
    "    gradients = list()\n",
    "    n = float(len(y))\n",
    "    for j in range(len(w)):\n",
    "        gradients.append(np.sum(loss(X, w, y) * X[:, j]) / n)\n",
    "    return gradients\n",
    "\n",
    "def update(X, w, y, alpha=0.01):\n",
    "    return [t - alpha * g for t, g in zip(w, gradient(X, w, y))]\n",
    "\n",
    "def optimize(X, y, alpha=0.01, eta=10**-12, iterations=1000):\n",
    "    w = random_w(X.shape[1])\n",
    "    path = list()\n",
    "    for k in range(iterations):\n",
    "        SSL = np.sum(squared_loss(X, w, y))\n",
    "        new_w = update(X, w, y, alpha=alpha)\n",
    "        new_SSL = np.sum(squared_loss(X, new_w, y))\n",
    "        w = new_w\n",
    "        if k >= 5 and (new_SSL - SSL <= eta and new_SSL - SSL >= -eta):\n",
    "            path.append(new_SSL)\n",
    "            return w, path\n",
    "        if k % (iterations / 20) == 0:\n",
    "            path.append(new_SSL)\n",
    "    return w, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are our final coefficients: [9.1021032364625398, -34.670584233610896]\n",
      "Obtained walking on this path of squared loss [361123.23183664575, 23868.501317764763, 22488.529217177176, 22162.636426081921, 22085.673919924993, 22067.498533675891, 22063.206253164251, 22062.192592615967, 22061.953207584083, 22061.896674661308, 22061.883323904447, 22061.880171003446, 22061.879426417592, 22061.87925057696, 22061.879209050621, 22061.879199243798, 22061.879196927832, 22061.879196380894, 22061.879196251735, 22061.879196221227, 22061.879196220129]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.048\n",
    "w, path = optimize(X, y, alpha, eta=10**-12, iterations=25000)\n",
    "print('These are our final coefficients: {0}'.format(w))\n",
    "print('Obtained walking on this path of squared loss {0}'.format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
